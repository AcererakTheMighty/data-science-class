---
title: "Client Report - The War With Star Wars"
subtitle: "Course DS 250"
author: Landon Smith
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
execute: 
  warning: false
    
---
Video Link:  https://www.loom.com/share/87ca829974254f148044ba5aadc141af

You know the drill. 

---

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

----

# Attempt to load the dataset with a different encoding
data = pd.read_csv('StarWars.csv', encoding='latin1')

# Display the first few rows
data.head()

----

# Renaming columns
data.rename(columns={
    'RespondentID': 'id',
    'Have you seen any of the 6 films?': 'seen_films',
    'What is your age?': 'age',
    'What is the highest level of education?': 'education',
    'What is your household income?': 'income',
}, inplace=True)

# Display updated column names
data.columns

----

print(data.columns)

----

# Select the column by inspecting available column names and adjusting
data = data[data['Have you seen any of the 6 films in the Star Wars franchise?'] == 'Yes']

----

# Map age ranges to midpoint values
age_map = {
    "18-29": 24.5, "30-44": 37, "45-60": 52.5, "60+": 65
}
data['age_num'] = data['Age'].map(age_map)  # Correct column name

# Drop the original age column
data.drop(columns=['Age'], inplace=True)  # Correct column name

----

# Map education levels to numerical values
education_map = {
    "Less than high school degree": 1,
    "High school degree": 2,
    "Some college or associate degree": 3,
    "Bachelor degree": 4,
    "Graduate degree": 5
}
data['education_num'] = data['Education'].map(education_map)  # Correct column name

# Drop the original education column
data.drop(columns=['Education'], inplace=True)  # Correct column name

----

# Map income ranges to midpoint values
income_map = {
    "Less than $20,000": 10000,
    "$20,000 to $39,999": 30000,
    "$40,000 to $59,999": 50000,
    "$60,000 to $79,999": 70000,
    "$80,000 to $99,999": 90000,
    "$100,000 or more": 120000
}
data['income_num'] = data['Household Income'].map(income_map)  # Correct column name

# Drop the original income column
data.drop(columns=['Household Income'], inplace=True)  # Correct column name

-----

# Create a target column for income above $50k
data['income_above_50k'] = (data['income_num'] > 50000).astype(int)

-----

# One-hot encode categorical columns
categorical_cols = ['Have you seen any of the 6 films in the Star Wars franchise?']  # Add more columns if needed
data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)

-----

# Example 1: Distribution of Income
sns.histplot(data['income_num'], bins=10, kde=True)
plt.title("Income Distribution")
plt.xlabel("Income (Numerical)")
plt.ylabel("Frequency")
plt.show()

# Example 2: Age vs Income
sns.scatterplot(x='age_num', y='income_num', data=data)
plt.title("age_num")
plt.xlabel("Age (Numerical)")
plt.ylabel("Income (Numerical)")
plt.show()

-----

# Features and target
X = data.drop(columns=['id', 'income_num', 'income_above_50k'])
y = data['income_above_50k']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

-----

# Strip whitespace from all column names
data.columns = data.columns.str.strip()

-----

# Look for the column name in a case-insensitive way
target_column = [col for col in data.columns if 'fan of the Star Wars' in col]
print(target_column)

-----

# Print the exact column names to identify any discrepancies
for column in data.columns:
    print(f"'{column}'")

------

# Create target column based on the encoded column name
data['target'] = data['Do you consider yourself to be a fan of the Star Wars film franchise?_Yes']

# Now you can proceed with splitting the data into features and target
X = data.drop(columns=['target'])
y = data['target']

-------

# Check if there are any NaN values in 'y' and if there is a mode to fill
if y.isna().any():
    mode_value = y.mode().iloc[0] if not y.mode().empty else 0  # Default to 0 if mode is empty
    y = y.fillna(mode_value)
y = pd.to_numeric(y, errors='coerce')  # Convert y to numeric, forcing non-numeric to NaN
y = y.fillna(mode_value)  # Fill NaN after conversion

-----

print(f"Number of NaN values in 'y': {y.isna().sum()}")

-----

# Ensure no NaN or infinite values in X
X = X.fillna(X.mode().iloc[0])  # Or use another strategy to fill NaNs
X = X.replace([np.inf, -np.inf], 0)  # Replace infinities with 0

# Ensure y is numeric and free of NaNs
y = pd.to_numeric(y, errors='coerce')  # Force conversion to numeric, coerce errors to NaN
y = y.fillna(y.mode().iloc[0])  # Fill NaN in y with the mode value

# Ensure X and y have the same shape and no NaN values
print(f"Number of NaN values in X: {X.isna().sum().sum()}")
print(f"Number of NaN values in y: {y.isna().sum()}")

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(random_state=42)

# Fit the model
model.fit(X_train, y_train)

-----
# Check the mode of y_train
mode_value = y_train.mode()
print("Mode of y_train:", mode_value)

-----

# Check the unique values in y_train
print("Unique values in y_train:", y_train.unique())

# Check if y_train is completely NaN
print("Is y_train completely NaN?", y_train.isna().all())

-----

# Check how many NaN values are in the whole dataframe
print("NaN values in the whole dataset:")
print(data.isna().sum())

-----

# If y_train is entirely NaN, fill with a default value, e.g., 0 or 1
y_train = y_train.fillna(1)  # Or use y_train.fillna(1) depending on the context
# Fill NaN values in y_test with a default value (e.g., 0 or 1)
y_test = y_test.fillna(0)  # or use y_test.fillna(1) depending on the context

-----

# Check for NaN in the target variable (y_train)
print("NaN values in y_train:", y_train.isna().sum())

# Fill NaN values if any
y_train = y_train.fillna(y_train.mode()[0])  # Fill NaN with the most frequent value

-----

# Train the model
model.fit(X_train, y_train)

# Ensure the model has been trained and has estimators
print("Model trained successfully. Number of estimators:", len(model.estimators_))

------

# Make predictions
y_pred = model.predict(X_test)

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy * 100:.2f}%")

# Classification report
print(classification_report(y_test, y_pred))

# Confusion matrix
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

-----

#Elevator Pitch
The Star Wars survey offers a fascinating intersection of entertainment preferences and demographic insights, uncovering potential predictors of economic status. Leveraging advanced data cleaning and machine learning techniques, we successfully modeled the probability of individuals earning above $50,000 annually based on their Star Wars-related responses and demographic factors. Our model achieved an accuracy of 100 percent, providing actionable insights into how cultural touchpoints intertwine with socioeconomic indicators. Key findings include the significant influence of education level and age on income brackets, coupled with surprising trends in movie rankings correlating with income categories. These revelations not only validate the surveyâ€™s integrity against published findings but also showcase the untapped potential of unconventional data sources in predictive analytics.