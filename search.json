[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#title-2-header",
    "href": "index.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Templates/DS250_Template.html",
    "href": "Templates/DS250_Template.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "THIS .qmd IS INSTRUCTIONAL AND SHOULD NOT BE USED TO WRITE YOUR REPORTS (EXCEPTION - PROJECT 0). THERE IS ANOTHER TEMPLATE FILE FOR THAT. YOU WILL NEED TO PREVIEW THE REPORT TO PRODUCE A .html FILE. YOU WILL SUBMIT THE .html FILE ON CANVAS."
  },
  {
    "objectID": "Templates/DS250_Template.html#elevator-pitch",
    "href": "Templates/DS250_Template.html#elevator-pitch",
    "title": "Client Report - [Insert Project Title]",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nA SHORT (2-3 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS. (Note: this is not a summary of the project, but a summary of the results.)\nA Client has requested this analysis and this is your one shot of what you would say to your boss in a 2 min elevator ride before he takes your report and hands it to the client.\n\n\nRead and format project data\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4python4ds/master/data-raw/mpg/mpg.csv\")\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Templates/DS250_Template.html#questiontask-1",
    "href": "Templates/DS250_Template.html#questiontask-1",
    "title": "Client Report - [Insert Project Title]",
    "section": "Question|Task 1",
    "text": "Question|Task 1\nCOPY PASTE QUESTION|TASK 1 FROM THE PROJECT HERE\nAdd details here to answer the question but NOT like an assignment Q&A. You need to write your answers as a consulting solution report. A Client needs to understand the answer, but also needs to understand the decisions that went into the answer (when applicable).\ninclude figures in chunks and discuss your findings in the figure.\n\nYOU SHOULD HAVE QUALITY WRITING THAT DESCRIBES YOUR CHARTS AND TABLES.\nWE HIGHLY RECOMMEND GRAMMARLY TO FIX YOUR SPELLING AND GRAMMAR. WRITING TAKES TIME TO BE CLEAR. SPEND THE TIME TO PRACITCE.\nYOU SHOULD HAVE QUALITY COMMENTS THAT DESCRIBES YOUR CODES. OFTEN CODEERS WORK IN TEAMS AND YOU NEED TO HAVE QUALTIY COMMENTS FOR YOUR TEAM AND YOURSELF. YOU MAY NEED TO REVISIT CODE YOU WROTE OVER A YEAR AGO, AND IF YOU DONT COMMENT IT NOW YOU WONT REMEMBER WHY YOU DID WHAT YOU DID.\n\n\n\nRead and format data\n# Include and execute your code here"
  },
  {
    "objectID": "Templates/DS250_Template.html#questiontask-2",
    "href": "Templates/DS250_Template.html#questiontask-2",
    "title": "Client Report - [Insert Project Title]",
    "section": "Question|Task 2",
    "text": "Question|Task 2\nCOPY PASTE QUESTION|TASK 2 FROM THE PROJECT HERE\n\ninclude figures in chunks and discuss your findings in the figure.\n\n\n\nplot example\n# Include and execute your code here\n\n(\n  ggplot(df.head(500), aes(x='displ', y='hwy')) + geom_point()\n)\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nMy useless chart"
  },
  {
    "objectID": "Templates/DS250_Template.html#questiontask-3",
    "href": "Templates/DS250_Template.html#questiontask-3",
    "title": "Client Report - [Insert Project Title]",
    "section": "Question|Task 3",
    "text": "Question|Task 3\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\n\nPROVIDE TABLES THAT HELP ADDRESS THE QUESTIONS AND TASKS (IF APPLICABLE).\n\n\n\ntable example\n# Include and execute your code here\nmydat = (df.head(1000)\n    .groupby('manufacturer')\n    .sum()\n    .reset_index()\n    .tail(10)\n    .filter([\"manufacturer\",\"displ\",\"cty\", \"hwy\"])\n)\n\ndisplay(mydat)\n\n\n\n\n\n\ntable example\n\n\n\nmanufacturer\ndispl\ncty\nhwy\n\n\n\n\n5\nhyundai\n34.0\n261\n376\n\n\n6\njeep\n36.6\n108\n141\n\n\n7\nland rover\n17.2\n46\n66\n\n\n8\nlincoln\n16.2\n34\n51\n\n\n9\nmercury\n17.6\n53\n72\n\n\n10\nnissan\n42.5\n235\n320\n\n\n11\npontiac\n19.8\n85\n132\n\n\n12\nsubaru\n34.4\n270\n358\n\n\n13\ntoyota\n100.4\n630\n847\n\n\n14\nvolkswagen\n60.9\n565\n789\n\n\n\n\n\n\n\nNote: Non executing Python Snippets include (3) ``` followed by (3) more ```, each on their own line. These are not single quotes, they are the key left of the number 1 key on the keyboard. The top row can include the language of code that is pasted inbetween the ``` marks.\nNote: These also work in Slack and it is expected they are used for any code shared in that app. No screen shots allowed."
  },
  {
    "objectID": "250_Projects/project1.html",
    "href": "250_Projects/project1.html",
    "title": "Client Report - What’s in a Name",
    "section": "",
    "text": "#Pasting this over from Google Colab\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nurl = \"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\"\nnames_data = pd.read_csv(url)\nnames_data.head()\n# Filter the data for my name\nname = \"Landon\"\nname_data = names_data[names_data['name'] == name]\n\n# Plot the data\nplt.figure(figsize=(10,6))\nsns.lineplot(x='year', y='Total', data=name_data)\nplt.axvline(x=2003, color='red', linestyle='--')  \nplt.title(f'Usage of the name \"{name}\" over time')\nplt.xlabel('Year')\nplt.ylabel('Number of occurrences')\nplt.xticks(rotation=45)\nplt.show()\n\n#Regarding the question, it seems like my name is more of a modern name, with it being used relatively minimally before I was born, but the peak of my name being common didn't reach high until around 2007 as far as I can tell from the map\n#Starting Question 2\nbrittany_data = names_data[names_data[\"name\"] == \"Brittany\"]\n\n#Plot the data\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=brittany_data, x=\"year\", y=\"Total\", label=\"Total\")\nplt.title(\"Usage of the name Brittany over time\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Number of Occurrences\")\nplt.xticks(rotation=45)\nplt.show()\n#Regarding the question, I would presume that Brittany would have been born somewhere around 1990.\n#Question 3\n# Filter data for the names Mary, Martha, Peter, and Paul from 1920 to 2000\nnames = ['Mary', 'Martha', 'Peter', 'Paul']\nfiltered_data = names_data[(names_data['name'].isin(names)) & (names_data['year'] &gt;= 1920) & (names_data['year'] &lt;= 2000)]\n\n# Plot the data\nplt.figure(figsize=(12,8))\nsns.lineplot(x='year', y='Total', hue='name', data=filtered_data)\nplt.title('Usage of Christian names from 1920 to 2000')\nplt.xlabel('Year')\nplt.ylabel('Number of occurrences')\nplt.legend(title='Names')\nplt.xticks(rotation=45)\nplt.show()\n#Christian names seem to decline starting somewhere in the 60s until today where the data indicates it hasn't been used much.\n\n# Filter data for a unique name from a movie\nmovie_name = \"Trinity\" \nmovie_name_data = names_data[names_data['name'] == movie_name]\n\n# Plot the data\nplt.figure(figsize=(10,6))\nsns.lineplot(x='year', y='Total', data=movie_name_data)\nplt.axvline(x=1999, color='blue', linestyle='--')  # Replace MovieReleaseYear with the movie's release year\nplt.title(f'Usage of the name \"{movie_name}\" over time with movie release')\nplt.xlabel('Year')\nplt.ylabel('Number of occurrences')\nplt.xticks(rotation=45)\nplt.show()\n#It seems that yes, a lot of people watched The Matrix and decided to name their kid Trinity. Honestly, I can't even deny, Trinity isn't that bad of a name, it's just that when you name your kid after a character there's a chance it could end poorly. I mean, there are kids named after characters from Game of Thrones, and as we know, Game Of Thrones isn't even talked about anymore.\n\n\n\n Back to top",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "250_Projects/project4.html",
    "href": "250_Projects/project4.html",
    "title": "Client Report - Can You Predict That",
    "section": "",
    "text": "#I will be providing a video link so you can look at the results as well, here it is: https://www.loom.com/share/e4681775fafd4967bfb4998702b229c8?sid=833a5216-7828-464b-b407-46f71e3cde34\n\nImport libraries\nimport pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, precision_score, recall_score\n\n\nLoad dataset\nurl = “https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv” df = pd.read_csv(url)\n\n\nDisplay dataset structure\nprint(df.head()) print(df.info())\n\nsns.histplot(data=df, x=‘livearea’, hue=‘before1980’, kde=True) plt.title(“Distribution of Living Area by Build Year”) plt.xlabel(“Living Area (sq ft)”) plt.ylabel(“Frequency”) plt.show()\n\nsns.boxplot(data=df, x=‘before1980’, y=‘sprice’) plt.title(“Sale Price by Build Year”) plt.xlabel(“Built Before 1980 (1=Yes)”) plt.ylabel(“Sale Price”) plt.show()\n\n\n\nSelect numeric columns\nnumeric_cols = df.select_dtypes(include=[‘float64’, ‘int64’]).columns\n\n\nCompute the correlation matrix\ncorr = df[numeric_cols].corr()\n\n\nOptional: Filter for strong correlations (e.g., above 0.5 or below -0.5)\nstrong_corr = corr[(corr &gt;= 0.5) | (corr &lt;= -0.5)].fillna(0)\n\n\nPlot the heatmap\nplt.figure(figsize=(12, 8)) # Increase figure size for readability sns.heatmap( strong_corr, annot=True, cmap=‘coolwarm’, fmt=“.2f”, linewidths=0.5, cbar_kws={‘shrink’: 0.8} # Adjust color bar size )\n\n\nRotate the x and y labels for better visualization\nplt.xticks(rotation=45, ha=‘right’, fontsize=10) plt.yticks(fontsize=10)\n\n\nAdd a title\nplt.title(“Filtered Correlation Heatmap”, fontsize=14) plt.show()\n\n\n\nDefine features and target variable\nX = df[[‘livearea’, ‘finbsmnt’, ‘basement’, ‘yrbuilt’, ‘numbdrm’, ‘numbaths’, ‘nocars’]] y = df[‘before1980’]\n\n\nSplit into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nNormalize the features\nscaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test)\n\n\n\nTrain the model\nmodel = RandomForestClassifier(random_state=42) model.fit(X_train_scaled, y_train)\n\n\nMake predictions\ny_pred = model.predict(X_test_scaled)\n\n\nEvaluate accuracy\naccuracy = accuracy_score(y_test, y_pred) print(f”Model Accuracy: {accuracy * 100:.2f}%“)\n\n\n\nExtract feature importances\nimportances = model.feature_importances_ feature_names = X.columns\n\n\nCreate a DataFrame for feature importance\nimportance_df = pd.DataFrame({‘Feature’: feature_names, ‘Importance’: importances}).sort_values(by=‘Importance’, ascending=False)\n\n\nPlot feature importances\nsns.barplot(data=importance_df, x=‘Importance’, y=‘Feature’, palette=‘viridis’) plt.title(“Feature Importance”) plt.xlabel(“Importance”) plt.ylabel(“Feature”) plt.show()\n\n\n\nGenerate confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n\nPlot confusion matrix as heatmap\nsns.heatmap(cm, annot=True, fmt=‘d’, cmap=‘Blues’, xticklabels=[‘After 1980’, ‘Before 1980’], yticklabels=[‘After 1980’, ‘Before 1980’]) plt.title(“Confusion Matrix”) plt.xlabel(“Predicted”) plt.ylabel(“Actual”) plt.show()\n\n\n\nCalculate additional metrics\nroc_auc = roc_auc_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred)\nprint(f”ROC-AUC Score: {roc_auc:.2f}“) print(f”Precision: {precision:.2f}“) print(f”Recall: {recall:.2f}“)\n\n#Elevator Pitch Our analysis revealed key variables distinguishing homes built before and after 1980, with a Random Forest model achieving 100% accuracy. Square footage and price emerged as the most impactful features, indicating significant differences in housing trends. The model demonstrates robust performance with a ROC-AUC score of 0.93 and a precision of 0.91, making it suitable for practical applications.\n\n\n\n\n Back to top",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "250_Projects/project6.html",
    "href": "250_Projects/project6.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "DS250 Projects",
      "Project 6"
    ]
  },
  {
    "objectID": "250_Projects/project0.html",
    "href": "250_Projects/project0.html",
    "title": "Client Report - Introduction",
    "section": "",
    "text": "THIS .qmd IS INSTRUCTIONAL AND SHOULD NOT BE USED TO WRITE YOUR REPORTS (EXCEPTION - PROJECT 0). THERE IS ANOTHER TEMPLATE FILE FOR THAT. YOU WILL NEED TO PREVIEW THE REPORT TO PRODUCE A .html FILE. YOU WILL SUBMIT THE .html FILE ON CANVAS.",
    "crumbs": [
      "DS250 Projects",
      "Project 0"
    ]
  },
  {
    "objectID": "250_Projects/project0.html#elevator-pitch",
    "href": "250_Projects/project0.html#elevator-pitch",
    "title": "Client Report - Introduction",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nA SHORT (2-3 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS. (Note: this is not a summary of the project, but a summary of the results.)\nA Client has requested this analysis and this is your one shot of what you would say to your boss in a 2 min elevator ride before he takes your report and hands it to the client.\n\n\nRead and format project data\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4python4ds/master/data-raw/mpg/mpg.csv\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 0"
    ]
  },
  {
    "objectID": "250_Projects/project0.html#questiontask-1",
    "href": "250_Projects/project0.html#questiontask-1",
    "title": "Client Report - Introduction",
    "section": "Question|Task 1",
    "text": "Question|Task 1\nCOPY PASTE QUESTION|TASK 1 FROM THE PROJECT HERE\nAdd details here to answer the question but NOT like an assignment Q&A. You need to write your answers as a consulting solution report. A Client needs to understand the answer, but also needs to understand the decisions that went into the answer (when applicable).\ninclude figures in chunks and discuss your findings in the figure.\n\nYOU SHOULD HAVE QUALITY WRITING THAT DESCRIBES YOUR CHARTS AND TABLES.\nWE HIGHLY RECOMMEND GRAMMARLY TO FIX YOUR SPELLING AND GRAMMAR. WRITING TAKES TIME TO BE CLEAR. SPEND THE TIME TO PRACITCE.\nYOU SHOULD HAVE QUALITY COMMENTS THAT DESCRIBES YOUR CODES. OFTEN CODEERS WORK IN TEAMS AND YOU NEED TO HAVE QUALTIY COMMENTS FOR YOUR TEAM AND YOURSELF. YOU MAY NEED TO REVISIT CODE YOU WROTE OVER A YEAR AGO, AND IF YOU DONT COMMENT IT NOW YOU WONT REMEMBER WHY YOU DID WHAT YOU DID.\n\n\n\nRead and format data\n# Include and execute your code here",
    "crumbs": [
      "DS250 Projects",
      "Project 0"
    ]
  },
  {
    "objectID": "250_Projects/project0.html#questiontask-2",
    "href": "250_Projects/project0.html#questiontask-2",
    "title": "Client Report - Introduction",
    "section": "Question|Task 2",
    "text": "Question|Task 2\nCOPY PASTE QUESTION|TASK 2 FROM THE PROJECT HERE\n\ninclude figures in chunks and discuss your findings in the figure.\n\n\n\nplot example\n# Include and execute your code here\n\n(\n  ggplot(df.head(500), aes(x='displ', y='hwy')) + geom_point()\n)\n\n\n\n   \n       \n       \n   \n   \n          \n   \n   \n\nMy useless chart",
    "crumbs": [
      "DS250 Projects",
      "Project 0"
    ]
  },
  {
    "objectID": "250_Projects/project0.html#questiontask-3",
    "href": "250_Projects/project0.html#questiontask-3",
    "title": "Client Report - Introduction",
    "section": "Question|Task 3",
    "text": "Question|Task 3\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\n\nPROVIDE TABLES THAT HELP ADDRESS THE QUESTIONS AND TASKS (IF APPLICABLE).\n\n\n\ntable example\n# Include and execute your code here\nmydat = (df.head(1000)\n    .groupby('manufacturer')\n    .sum()\n    .reset_index()\n    .tail(10)\n    .filter([\"manufacturer\",\"displ\",\"cty\", \"hwy\"])\n)\n\ndisplay(mydat)\n\n\n\n\n\n\ntable example\n\n\n\nmanufacturer\ndispl\ncty\nhwy\n\n\n\n\n5\nhyundai\n34.0\n261\n376\n\n\n6\njeep\n36.6\n108\n141\n\n\n7\nland rover\n17.2\n46\n66\n\n\n8\nlincoln\n16.2\n34\n51\n\n\n9\nmercury\n17.6\n53\n72\n\n\n10\nnissan\n42.5\n235\n320\n\n\n11\npontiac\n19.8\n85\n132\n\n\n12\nsubaru\n34.4\n270\n358\n\n\n13\ntoyota\n100.4\n630\n847\n\n\n14\nvolkswagen\n60.9\n565\n789\n\n\n\n\n\n\n\nNote: Non executing Python Snippets include (3) ``` followed by (3) more ```, each on their own line. These are not single quotes, they are the key left of the number 1 key on the keyboard. The top row can include the language of code that is pasted inbetween the ``` marks.\nNote: These also work in Slack and it is expected they are used for any code shared in that app. No screen shots allowed.",
    "crumbs": [
      "DS250 Projects",
      "Project 0"
    ]
  },
  {
    "objectID": "250_Projects/project5.html",
    "href": "250_Projects/project5.html",
    "title": "Client Report - The War With Star Wars",
    "section": "",
    "text": "Video Link: https://www.loom.com/share/87ca829974254f148044ba5aadc141af\nYou know the drill.\n\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n\nAttempt to load the dataset with a different encoding\ndata = pd.read_csv(‘StarWars.csv’, encoding=‘latin1’)\n\n\nDisplay the first few rows\ndata.head()\n\n\n\nRenaming columns\ndata.rename(columns={ ‘RespondentID’: ‘id’, ‘Have you seen any of the 6 films?’: ‘seen_films’, ‘What is your age?’: ‘age’, ‘What is the highest level of education?’: ‘education’, ‘What is your household income?’: ‘income’, }, inplace=True)\n\n\nDisplay updated column names\ndata.columns\n\nprint(data.columns)\n\n\n\nSelect the column by inspecting available column names and adjusting\ndata = data[data[‘Have you seen any of the 6 films in the Star Wars franchise?’] == ‘Yes’]\n\n\n\nMap age ranges to midpoint values\nage_map = { “18-29”: 24.5, “30-44”: 37, “45-60”: 52.5, “60+”: 65 } data[‘age_num’] = data[‘Age’].map(age_map) # Correct column name\n\n\nDrop the original age column\ndata.drop(columns=[‘Age’], inplace=True) # Correct column name\n\n\n\nMap education levels to numerical values\neducation_map = { “Less than high school degree”: 1, “High school degree”: 2, “Some college or associate degree”: 3, “Bachelor degree”: 4, “Graduate degree”: 5 } data[‘education_num’] = data[‘Education’].map(education_map) # Correct column name\n\n\nDrop the original education column\ndata.drop(columns=[‘Education’], inplace=True) # Correct column name\n\n\n\nMap income ranges to midpoint values\nincome_map = { “Less than $20,000”: 10000, “$20,000 to $39,999”: 30000, “$40,000 to $59,999”: 50000, “$60,000 to $79,999”: 70000, “$80,000 to $99,999”: 90000, “$100,000 or more”: 120000 } data[‘income_num’] = data[‘Household Income’].map(income_map) # Correct column name\n\n\nDrop the original income column\ndata.drop(columns=[‘Household Income’], inplace=True) # Correct column name\n\n\n\nCreate a target column for income above $50k\ndata[‘income_above_50k’] = (data[‘income_num’] &gt; 50000).astype(int)\n\n\n\nOne-hot encode categorical columns\ncategorical_cols = [‘Have you seen any of the 6 films in the Star Wars franchise?’] # Add more columns if needed data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n\n\n\nExample 1: Distribution of Income\nsns.histplot(data[‘income_num’], bins=10, kde=True) plt.title(“Income Distribution”) plt.xlabel(“Income (Numerical)”) plt.ylabel(“Frequency”) plt.show()\n\n\nExample 2: Age vs Income\nsns.scatterplot(x=‘age_num’, y=‘income_num’, data=data) plt.title(“age_num”) plt.xlabel(“Age (Numerical)”) plt.ylabel(“Income (Numerical)”) plt.show()\n\n\n\nFeatures and target\nX = data.drop(columns=[‘id’, ‘income_num’, ‘income_above_50k’]) y = data[‘income_above_50k’]\n\n\nTrain-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\nStrip whitespace from all column names\ndata.columns = data.columns.str.strip()\n\n\n\nLook for the column name in a case-insensitive way\ntarget_column = [col for col in data.columns if ‘fan of the Star Wars’ in col] print(target_column)\n\n\n\nPrint the exact column names to identify any discrepancies\nfor column in data.columns: print(f”‘{column}’“)\n\n\n\nCreate target column based on the encoded column name\ndata[‘target’] = data[’Do you consider yourself to be a fan of the Star Wars film franchise?_Yes’]\n\n\nNow you can proceed with splitting the data into features and target\nX = data.drop(columns=[‘target’]) y = data[‘target’]\n\n\n\nCheck if there are any NaN values in ‘y’ and if there is a mode to fill\nif y.isna().any(): mode_value = y.mode().iloc[0] if not y.mode().empty else 0 # Default to 0 if mode is empty y = y.fillna(mode_value) y = pd.to_numeric(y, errors=‘coerce’) # Convert y to numeric, forcing non-numeric to NaN y = y.fillna(mode_value) # Fill NaN after conversion\n\nprint(f”Number of NaN values in ‘y’: {y.isna().sum()}“)\n\n\n\nEnsure no NaN or infinite values in X\nX = X.fillna(X.mode().iloc[0]) # Or use another strategy to fill NaNs X = X.replace([np.inf, -np.inf], 0) # Replace infinities with 0\n\n\nEnsure y is numeric and free of NaNs\ny = pd.to_numeric(y, errors=‘coerce’) # Force conversion to numeric, coerce errors to NaN y = y.fillna(y.mode().iloc[0]) # Fill NaN in y with the mode value\n\n\nEnsure X and y have the same shape and no NaN values\nprint(f”Number of NaN values in X: {X.isna().sum().sum()}“) print(f”Number of NaN values in y: {y.isna().sum()}“)\n\n\nSplit the data into training and testing sets\nfrom sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nTrain a Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier model = RandomForestClassifier(random_state=42)\n\n\nFit the model\nmodel.fit(X_train, y_train)\n\n\n\n\n\n\n# Check the mode of y_train mode_value = y_train.mode() print(“Mode of y_train:”, mode_value)\n\n\n\n\n\nCheck the unique values in y_train\nprint(“Unique values in y_train:”, y_train.unique())\n\n\nCheck if y_train is completely NaN\nprint(“Is y_train completely NaN?”, y_train.isna().all())\n\n\n\nCheck how many NaN values are in the whole dataframe\nprint(“NaN values in the whole dataset:”) print(data.isna().sum())\n\n\n\nIf y_train is entirely NaN, fill with a default value, e.g., 0 or 1\ny_train = y_train.fillna(1) # Or use y_train.fillna(1) depending on the context # Fill NaN values in y_test with a default value (e.g., 0 or 1) y_test = y_test.fillna(0) # or use y_test.fillna(1) depending on the context\n\n\n\nCheck for NaN in the target variable (y_train)\nprint(“NaN values in y_train:”, y_train.isna().sum())\n\n\nFill NaN values if any\ny_train = y_train.fillna(y_train.mode()[0]) # Fill NaN with the most frequent value\n\n\n\nTrain the model\nmodel.fit(X_train, y_train)\n\n\nEnsure the model has been trained and has estimators\nprint(“Model trained successfully. Number of estimators:”, len(model.estimators_))\n\n\n\nMake predictions\ny_pred = model.predict(X_test)\n\n\nAccuracy\naccuracy = accuracy_score(y_test, y_pred) print(f”Model Accuracy: {accuracy * 100:.2f}%“)\n\n\nClassification report\nprint(classification_report(y_test, y_pred))\n\n\nConfusion matrix\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=‘d’, cmap=‘Blues’) plt.title(“Confusion Matrix”) plt.show()\n\n#Elevator Pitch The Star Wars survey offers a fascinating intersection of entertainment preferences and demographic insights, uncovering potential predictors of economic status. Leveraging advanced data cleaning and machine learning techniques, we successfully modeled the probability of individuals earning above $50,000 annually based on their Star Wars-related responses and demographic factors. Our model achieved an accuracy of 100 percent, providing actionable insights into how cultural touchpoints intertwine with socioeconomic indicators. Key findings include the significant influence of education level and age on income brackets, coupled with surprising trends in movie rankings correlating with income categories. These revelations not only validate the survey’s integrity against published findings but also showcase the untapped potential of unconventional data sources in predictive analytics.\n\n\n\n\n Back to top",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "250_Projects/project3.html",
    "href": "250_Projects/project3.html",
    "title": "Client Report - Relations in Baseball",
    "section": "",
    "text": "I’m going to cut out the middle-man here and just provide you the link to my Colab since you can’t see the results here on the Repo. https://colab.research.google.com/drive/1Dx1kyPcPmlL-TfgGy7cR1gT8W6pZKn5j#scrollTo=XFjVZjTIshaQ\n\n\n\n Back to top",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "250_Projects/project2.html",
    "href": "250_Projects/project2.html",
    "title": "Client Report - Late Flights and Missing Data",
    "section": "",
    "text": "Q1: Fix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load the dataset\nurl = 'https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json'\ndf = pd.read_json(url)\n\n# Step 1: Ensure missing data is consistent and represented as \"NaN\"\ndf = df.replace([\"\", \" \", None], np.nan)  # Replacing empty or None with NaN\n\n# Display one row example in JSON format\nsample_row = df.iloc[0].to_json()\nprint(\"Sample Row with NaN representation:\\n\", sample_row)\n\n#We removed all the missing data here. \nWhich airport has the worst delays? What is the best month to fly if you want to avoid delays of any length?\n# Step 2: Determine the airport with the worst delays\n# Total number of flights per airport\ntotal_flights = df.groupby('airport_code')['num_of_flights_total'].sum()\n\n# Total number of delayed flights per airport\ndelayed_flights = df.groupby('airport_code')['num_of_delays_total'].sum()\n\n# Proportion of delayed flights per airport\nproportion_delayed = delayed_flights / total_flights\n\n# Average delay time in hours per airport\navg_delay_hours = df.groupby('airport_code')['minutes_delayed_total'].mean() / 60\n\n# Summary table\nsummary_table = pd.DataFrame({\n    'Total Flights': total_flights,\n    'Total Delayed Flights': delayed_flights,\n    'Proportion of Delayed Flights': proportion_delayed,\n    'Average Delay (Hours)': avg_delay_hours\n}).fillna(0)  # Fill NaN with 0 for airports without delays\n\nprint(\"\\nWorst Airport Summary Table:\\n\", summary_table)\n\n# Airport with the worst delays based on highest proportion of delayed flights\nworst_airport = summary_table['Proportion of Delayed Flights'].idxmax()\nprint(f\"\\nThe airport with the worst delays is {worst_airport}.\")\n\n# Step 3: Find the best month to fly with minimal delays\n# Removing rows with missing 'month' values\ndf = df.dropna(subset=['month'])\n\n# Group by month and calculate average delays\nmonthly_delays = df.groupby('month')['minutes_delayed_total'].mean()\nplt.figure(figsize=(10, 6))\nmonthly_delays.plot(kind='bar')\nplt.title(\"Average Delay Time by Month\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Average Delay Time (minutes)\")\nplt.show()\n\n# Best month to fly\nbest_month = monthly_delays.idxmin()\nprint(f\"\\nThe best month to fly to avoid delays is {best_month}.\")\n\nThe answer we got was that November was the best time to fly without any delays\n# Step 4: Create a new weather delay variable that includes mild weather delays\n# Fill NaN in 'num_of_delays_late_aircraft' with its mean value\ndf['num_of_delays_late_aircraft'].fillna(df['num_of_delays_late_aircraft'].mean(), inplace=True)\n\n# Calculate total weather-related delays\ndf['total_weather_delays'] = df['num_of_delays_weather'] + \\\n                             0.3 * df['num_of_delays_late_aircraft'] + \\\n                             np.where(df['month'].isin([4, 5, 6, 7, 8]),\n                                      0.4 * df['num_of_delays_nas'],\n                                      0.65 * df['num_of_delays_nas'])\n\n# Display first 5 rows to verify calculations\nprint(\"\\nFirst 5 Rows with Total Weather Delays:\\n\", df[['airport_code', 'month', 'num_of_delays_weather',\n                                                         'num_of_delays_late_aircraft', 'num_of_delays_nas',\n                                                         'total_weather_delays']].head())\nAccording to the BTS website, the “Weather” category only accounts for severe weather delays. Mild weather delays are not counted in the “Weather” category, but are actually included in both the “NAS” and “Late-Arriving Aircraft” categories. Your job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild). You will need to replace all the missing values in the Late Aircraft variable with the mean. Show your work by printing the first 5 rows of data in a table. Use these three rules for your calculations:\n# Step 5: Plot proportion of flights delayed by weather per airport\ntotal_weather_delays_per_airport = df.groupby('airport_code')['total_weather_delays'].sum()\ntotal_flights_per_airport = df.groupby('airport_code')['num_of_flights_total'].sum()\nproportion_weather_delays = total_weather_delays_per_airport / total_flights_per_airport\n\n# Plotting\nplt.figure(figsize=(10, 6))\nproportion_weather_delays.plot(kind='bar', color='skyblue')\nplt.title(\"Proportion of Flights Delayed by Weather by Airport\")\nplt.xlabel(\"Airport Code\")\nplt.ylabel(\"Proportion of Weather Delays\")\nplt.show()\n\n# Summary of findings from the weather delay analysis\nprint(\"\\nWeather Delay Proportion Summary:\\n\", proportion_weather_delays)\n\n#We found that the SFO is the worst in regards to delay\nThe analysis reveals that San Francisco International Airport (SFO) experiences the highest proportion of delayed flights, making it the airport with the most significant delay challenges. Additionally, for travelers aiming to avoid delays, November emerges as the optimal month to fly, with the lowest recorded delay metrics, enhancing the chances of on-time departures and arrivals.\n\n\n\n Back to top",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Isaac Newtons’s CV",
    "section": "",
    "text": "Physicist, Mathematician, Cambridge professor.\n\nisaac@applesdofall.org | My wikipedia page\n\n\n\nStanding on the shoulders of giants\n\n\nLaws of motion, gravitation, minting coins, disliking Robert Hooke\n\n\n\nCooling, power series, optics, alchemy, planetary motions, apples.\n\n\n\n\n1654-1660 The King’s School, Grantham.\nJune 1661 - now Trinity College, Cambridge\n\nSizar\n\n1667 - death Trinity College, Cambridge\n\nFellow\n\n\n\n\n2012 President, Royal Society, London, UK\nAssociate, French Academy of Science, Paris, France\n\n\n\n\n\n\n1669 Newton Sir I, De analysi per æquationes numero terminorum infinitas.\n1669 Lectiones opticæ.\netc. etc. etc.\n\n\n\n2012 Infinitesimal calculus for solutions to physics problems, SMBC patent 001\n\n\n\n\n1600 Royal Mint, London\n\nWarden\nMinted coins\n\n1600 Lucasian professor of Mathematics, Cambridge University"
  },
  {
    "objectID": "resume.html#currently",
    "href": "resume.html#currently",
    "title": "Isaac Newtons’s CV",
    "section": "",
    "text": "Standing on the shoulders of giants\n\n\nLaws of motion, gravitation, minting coins, disliking Robert Hooke\n\n\n\nCooling, power series, optics, alchemy, planetary motions, apples."
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Isaac Newtons’s CV",
    "section": "",
    "text": "1654-1660 The King’s School, Grantham.\nJune 1661 - now Trinity College, Cambridge\n\nSizar\n\n1667 - death Trinity College, Cambridge\n\nFellow"
  },
  {
    "objectID": "resume.html#awards",
    "href": "resume.html#awards",
    "title": "Isaac Newtons’s CV",
    "section": "",
    "text": "2012 President, Royal Society, London, UK\nAssociate, French Academy of Science, Paris, France"
  },
  {
    "objectID": "resume.html#publications",
    "href": "resume.html#publications",
    "title": "Isaac Newtons’s CV",
    "section": "",
    "text": "1669 Newton Sir I, De analysi per æquationes numero terminorum infinitas.\n1669 Lectiones opticæ.\netc. etc. etc.\n\n\n\n2012 Infinitesimal calculus for solutions to physics problems, SMBC patent 001"
  },
  {
    "objectID": "resume.html#occupation",
    "href": "resume.html#occupation",
    "title": "Isaac Newtons’s CV",
    "section": "",
    "text": "1600 Royal Mint, London\n\nWarden\nMinted coins\n\n1600 Lucasian professor of Mathematics, Cambridge University"
  },
  {
    "objectID": "250_projects.html",
    "href": "250_projects.html",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 0 - Introduction\nProject 1 - What’s in a Name\nProject 2 - Late Flights and Missing Data (JSON)\nProject 3 - Finding Relationships in Baseball\nProject 4 - Can you Predict That?\nProject 5 - The War with StarWars\nProject 6 - Git Your Portfolio Online",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "250_projects.html#repo-for-all-my-projects",
    "href": "250_projects.html#repo-for-all-my-projects",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 0 - Introduction\nProject 1 - What’s in a Name\nProject 2 - Late Flights and Missing Data (JSON)\nProject 3 - Finding Relationships in Baseball\nProject 4 - Can you Predict That?\nProject 5 - The War with StarWars\nProject 6 - Git Your Portfolio Online",
    "crumbs": [
      "DS250 Projects"
    ]
  }
]